{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git - Version Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Git is Version Control System (VCS) used to track changes in the project.\n",
    "\n",
    "Git is a Distributed Version Control System.\n",
    "\n",
    "It coordinates work between multiple developers and coordinates changes.\n",
    "\n",
    "You have local repository in your system and you makes changes to the files in the local repository and push it to remote repository like GitHub/Bitbucket.\n",
    "\n",
    "git takes snapshots of the files with **'commit'** command.\n",
    "\n",
    "You can visit any snapshot at any time.\n",
    "\n",
    "You can put your files in staging area before commit with **'add'** command.\n",
    "\n",
    "Once you make commit to the remote repository other developers can **'pull'** that information onto their machines.\n",
    "\n",
    "You can also create branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$ git init** This command initilizes a local git repository. Go to a folder and execute this command. It creates .git file (hidden file) in that folder.\n",
    "\n",
    "**$ git add < file_name >**  Then after initialzing you can add the files in that folder to staging area. we use this command to add files to the staging area.\n",
    "\n",
    "**$ git status** to see the files in staging area.\n",
    "\n",
    "**$ git commit** takes everything in staging area and puts into local repository.\n",
    "\n",
    "**$ git push** takes your local repository and puts it in the remote repository.\n",
    "\n",
    "**$ git pull** pulls latest changes from remote repository to local repository.\n",
    "\n",
    "**$ git clone** clones remote repository to local folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use git from both command prompt and git bash (Recommended).\n",
    "\n",
    "**$ git rm --cached < file_name >** removes file from staging area.\n",
    "\n",
    "**$ git add * .html** adds all files ending with .html\n",
    "\n",
    "**$ git commit** you will get an editor, type the comment in that file and exit (:wq)\n",
    "\n",
    "**$ git commit -m 'comment'** eliminate commenting in editor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.gitignore** Create a file named .gitignore and include the files that you dont want to be in the staging area and add all the files.\n",
    "\n",
    "When you add all the files to stagin area, you will see every file when git status command is executed except the files added in the .gitignore file.\n",
    "\n",
    "**$ git branch < branch name >** creates branches.\n",
    "\n",
    "**$ git checkout < branch name >** to switch the current branch.\n",
    "\n",
    "**NOTE:** Files that you created in the branches doesnt show up in the master unless you merge them with master.\n",
    "\n",
    "**$ git remote** shows available remote repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create virtual environements in Anaconda or else python has standard library to create virtual environments called venv.\n",
    "\n",
    "If you dont have virtual environment, when you install a package like numpy it installs globally. When you have two applications running and two application needs different versions of the same package, then ther will be problem. \n",
    "\n",
    "Instead what you can do is you can create virtual environement for each of the application and run the application the specified virtual environement. \n",
    "\n",
    "The downside with the venv module is it cannot create virtual environments for other python versions than the host python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different approches to putting Machine Learning model into production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train** One Off, Batch and real-time/Online training.\n",
    "\n",
    "**Serve** Batch, Realtime (Database Trigger, Pub/Sub, Web-Service, inApp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Off** Models dont necessarily need to be trained continously. They will be trained whenever their performance is deteriorated and again pushed back to the production.\n",
    "\n",
    "**Batch Training** it allows to have constantly refreshed version of your model based on the latest train.\n",
    "\n",
    "Batch training can benefit lot from AutoML types of frameworks. AutoML enables you to perform/automate activities such as feature processing, feature selection, model selections and parameter optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python pickle module is used to Serialize objects so that they can be saved to a file, and loaded in a program again later.\n",
    "\n",
    "**What is pickling**\n",
    "\n",
    "Pickling is used for Serializing and Deserializing python object Structures. Serialization refers to process of converting an object in Memory to a byte stream that can be stored on disk or sent over network.\n",
    "\n",
    "later on this character stream can be retrieved and deserialized back to python object.\n",
    "\n",
    "Pickling is not confused with Compression. Pickling is conversion of an object in RAM to Disk. While compression is process of encoding data in fewer bytes to save disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What can you do with pickling**\n",
    "\n",
    "Send data on the TCP or socket connection.\n",
    "\n",
    "Save Machine Learning algorithms and use them again to make predictions later without having to train model again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When not to use pickle**\n",
    "\n",
    "When you use different programming languages pickle is not recommended, its protocol to specific to python.\n",
    "\n",
    "Pickle does not compatible with different versions of python. Unpickling a file that was pickled in different version of python doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What can be pickled**\n",
    "\n",
    "Booleans, integers, floats, complex numbers, strings, tuples, lists, sets, dictionaries and also Classes and Functions.\n",
    "\n",
    "**What cannot be pickled**\n",
    "\n",
    "Generators, innerclasses, lambda functions and different dicts cannot be pickled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickling a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling\n",
    "\n",
    "import pickle\n",
    "example_dict = {1:'one', 2:'two', 3:'three', 4:'four'}\n",
    "\n",
    "# opening a dict.pickle pickle file with write mode\n",
    "pickle_out = open(\"dict.pickle\", 'wb') # wb for write bytes\n",
    "\n",
    "# writing whatever is in the example_dict dictionary onto the pickle file\n",
    "pickle.dump(example_dict, pickle_out)\n",
    "\n",
    "# closing the pickle file\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'one', 2: 'two', 3: 'three', 4: 'four'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_in = open('dict.pickle', 'rb')\n",
    "pkl_example_dict = pickle.load(pickle_in)\n",
    "pkl_example_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickling a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\kalya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=20, multi_class='warn', n_jobs=3,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset\n",
    "data = load_iris()\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(data.data, data.target, test_size=0.3, random_state=4)\n",
    "\n",
    "# create model\n",
    "model = LogisticRegression(C=0.1,\n",
    "                          max_iter=20,\n",
    "                          fit_intercept=True,\n",
    "                          n_jobs=3,\n",
    "                          solver='liblinear')\n",
    "\n",
    "# Fit the model on training set\n",
    "model.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Linearregression.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score is 91.11111111111111\n"
     ]
    }
   ],
   "source": [
    "# We trained our model with pickle and let us test with pickle \n",
    "\n",
    "# some time later ......\n",
    "\n",
    "# Read the model from the disk\n",
    "\n",
    "pickle_in = open('Linearregression.pickle', 'rb')\n",
    "clf = pickle.load(pickle_in)\n",
    "\n",
    "score = clf.score(Xtest, Ytest)\n",
    "print(f\"Test Score is {100*score}\")\n",
    "\n",
    "Ypredict = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In production environment pickle saves you lot of time for training the algorithms. Train the model once store the pickle files and retrain the model whenever it is necessary or when model deteriorates its performance.\n",
    "\n",
    "One more thing you can do is you can scale your EC2 instances when training the model, then train the model on computationally high EC2 instances and kill the instances after the training is done and save the pickles in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joblib - Alternative for pickling for handling large numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joblib library is intended to be replacement for Pickle, for objects containing large data.\n",
    "\n",
    "joblib offers simple workflow when compared to pickle.\n",
    "\n",
    "Joblib also allows different compression methods such as 'zlib', 'gzip', 'bz2' and different levels of compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joblib_model.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib_file = \"joblib_model.pkl\"\n",
    "joblib.dump(model, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score is 91.11111111111111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Some time later...\n",
    "\n",
    "\n",
    "# loading from the file\n",
    "joblib_model = joblib.load(joblib_file)\n",
    "\n",
    "# using the model to predict\n",
    "score = joblib_model.score(Xtest, Ytest)\n",
    "print(f\"Test Score is {100*score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages of both Joblib and Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Version Compatability** The file that is pickled in one version of python cannot be unpickled using different version.\n",
    "\n",
    "**Model compatibility** One of the most frequent mistakes is saving your model with Pickle and Joblib, then changing the model before trying to restore from file. The internal structure of the model needs to stay unchanged between save and reload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving model via REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/value-stream-design/architecting-a-scalable-real-time-learning-system-95623d27dd15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
